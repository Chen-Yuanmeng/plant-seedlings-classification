lr -> 3.5e-05
    Epoch 8/8 | Train 0.4180/0.9710 | Val 0.4081/0.9779
lr -> 4.5e-05
    Epoch 8/8 | Train 0.4026/0.9720 | Val 0.3995/0.9811
lr -> 5.5e-05
    Epoch 8/8 | Train 0.3887/0.9786 | Val 0.3880/0.9821
lr -> 6.5e-05
    Epoch 8/8 | Train 0.3782/0.9800 | Val 0.3868/0.9821

layer-decay -> 0.6
    Epoch 8/8 | Train 0.4183/0.9720 | Val 0.4109/0.9789
layer-decay -> 0.7
    Epoch 8/8 | Train 0.3887/0.9786 | Val 0.3880/0.9821
layer-decay -> 0.8
    Epoch 8/8 | Train 0.3542/0.9834 | Val 0.3678/0.9811

batch-size -> 16
    Epoch 8/8 | Train 0.3810/0.9778 | Val 0.3792/0.9832
batch-size -> 24
    Epoch 8/8 | Train 0.3887/0.9786 | Val 0.3880/0.9821
batch-size -> 32
    Epoch 8/8 | Train 0.3912/0.9788 | Val 0.3964/0.9821

warmup-epochs -> 1
    Epoch 8/8 | Train 0.3887/0.9786 | Val 0.3880/0.9821
warmup-epochs -> 2
    Epoch 8/8 | Train 0.3930/0.9749 | Val 0.3929/0.9800

label-smoothing -> 0.05
    Epoch 8/8 | Train 0.3887/0.9786 | Val 0.3880/0.9821
label-smoothing -> 0.1
    Epoch 8/8 | Train 0.6154/0.9776 | Val 0.6199/0.9789


Results:
lr: (5e-5 to 7e-5, step 5e-6) 
layer-decay: (0.65 to 0.8, step 0.05)
batch-size: 16
warmup-epochs: 1
label-smoothing: (0.025 to 0.075, step 0.025)
